---
layout: post
title: "[DL] 04. NLP - 벡터화(Vectorization)"
subtitle: "BoW와 TF-IDF, spacy, sklearn 활용한 TF-IDF 실습"
date: 2021-04-14 10:45:13
background: '/img/posts/DL_01_텍스트마이닝사진.png'
categories: ['Deep Learning']
---



</style></head><body><article id="0f68b3a0-0a39-4726-ae04-b54db66d6e90" class="page sans"><header><h1 class="page-title">벡터라이제이션</h1><table class="properties"><tbody><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesDate"><path d="M10.8889,5.5 L3.11111,5.5 L3.11111,7.05556 L10.8889,7.05556 L10.8889,5.5 Z M12.4444,1.05556 L11.6667,1.05556 L11.6667,0 L10.1111,0 L10.1111,1.05556 L3.88889,1.05556 L3.88889,0 L2.33333,0 L2.33333,1.05556 L1.55556,1.05556 C0.692222,1.05556 0.00777777,1.75556 0.00777777,2.61111 L0,12.5 C0,13.3556 0.692222,14 1.55556,14 L12.4444,14 C13.3,14 14,13.3556 14,12.5 L14,2.61111 C14,1.75556 13.3,1.05556 12.4444,1.05556 Z M12.4444,12.5 L1.55556,12.5 L1.55556,3.94444 L12.4444,3.94444 L12.4444,12.5 Z M8.55556,8.61111 L3.11111,8.61111 L3.11111,10.1667 L8.55556,10.1667 L8.55556,8.61111 Z"></path></svg></span>게시일</th><td></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>난이도</th><td></td></tr><tr class="property-row property-row-person"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesPerson"><path d="M9.625,10.8465 C8.91187,10.2891 8.12088,9.926 7,9.26013 L7,8.71938 C7.21175,8.47612 7.392,8.176 7.53813,7.83213 C7.94587,7.7315 8.3125,7.33425 8.3125,7 C8.3125,6.51788 8.1095,6.32713 7.8715,6.17137 C7.8715,6.15562 7.875,6.14162 7.875,6.125 C7.875,5.41362 7.4375,3.5 5.25,3.5 C3.0625,3.5 2.625,5.4145 2.625,6.125 C2.625,6.14162 2.6285,6.15562 2.6285,6.17137 C2.3905,6.32713 2.1875,6.51788 2.1875,7 C2.1875,7.33425 2.55413,7.7315 2.96187,7.833 C3.108,8.176 3.28825,8.47612 3.5,8.71938 L3.5,9.26013 C2.37912,9.92513 1.58812,10.2882 0.875,10.8465 C0.041125,11.4984 0,12.4688 0,14 L10.5,14 C10.5,12.4688 10.4589,11.4984 9.625,10.8465 Z M13.125,7.3465 C12.4119,6.78912 11.6209,6.426 10.5,5.76013 L10.5,5.21938 C10.7118,4.97613 10.892,4.676 11.0381,4.33213 C11.4459,4.2315 11.8125,3.83425 11.8125,3.5 C11.8125,3.01787 11.6095,2.82713 11.3715,2.67138 C11.3715,2.65562 11.375,2.64162 11.375,2.625 C11.375,1.91363 10.9375,0 8.75,0 C6.5625,0 6.125,1.9145 6.125,2.625 C6.125,2.64162 6.1285,2.65562 6.1285,2.67138 C6.11188,2.68275 6.09787,2.69588 6.08125,2.70725 C7.83212,3.066 8.59688,4.54825 8.72813,5.74787 C8.97575,6.00863 9.1875,6.39625 9.1875,7 C9.1875,7.60288 8.771,8.20312 8.18388,8.51462 C8.127,8.624 8.06662,8.729 8.00275,8.82962 C8.155,8.91537 8.30025,8.99675 8.44025,9.07463 C9.08075,9.4325 9.63375,9.74137 10.164,10.1561 C10.3022,10.2638 10.4204,10.3801 10.5289,10.4991 L14,10.4991 C14,8.96875 13.9589,7.99837 13.125,7.3465 Z"></path></svg></span>담당자</th><td></td></tr><tr class="property-row property-row-date"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesDate"><path d="M10.8889,5.5 L3.11111,5.5 L3.11111,7.05556 L10.8889,7.05556 L10.8889,5.5 Z M12.4444,1.05556 L11.6667,1.05556 L11.6667,0 L10.1111,0 L10.1111,1.05556 L3.88889,1.05556 L3.88889,0 L2.33333,0 L2.33333,1.05556 L1.55556,1.05556 C0.692222,1.05556 0.00777777,1.75556 0.00777777,2.61111 L0,12.5 C0,13.3556 0.692222,14 1.55556,14 L12.4444,14 C13.3,14 14,13.3556 14,12.5 L14,2.61111 C14,1.75556 13.3,1.05556 12.4444,1.05556 Z M12.4444,12.5 L1.55556,12.5 L1.55556,3.94444 L12.4444,3.94444 L12.4444,12.5 Z M8.55556,8.61111 L3.11111,8.61111 L3.11111,10.1667 L8.55556,10.1667 L8.55556,8.61111 Z"></path></svg></span>마감일</th><td></td></tr><tr class="property-row property-row-select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesSelect"><path d="M7,13 C10.31348,13 13,10.31371 13,7 C13,3.68629 10.31348,1 7,1 C3.68652,1 1,3.68629 1,7 C1,10.31371 3.68652,13 7,13 Z M3.75098,5.32278 C3.64893,5.19142 3.74268,5 3.90869,5 L10.09131,5 C10.25732,5 10.35107,5.19142 10.24902,5.32278 L7.15771,9.29703 C7.07764,9.39998 6.92236,9.39998 6.84229,9.29703 L3.75098,5.32278 Z"></path></svg></span>상태</th><td></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>유형</th><td></td></tr><tr class="property-row property-row-url"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesUrl"><path d="M3.73333,3.86667 L7.46667,3.86667 C8.49613,3.86667 9.33333,4.70387 9.33333,5.73333 C9.33333,6.7628 8.49613,7.6 7.46667,7.6 L6.53333,7.6 C6.01813,7.6 5.6,8.0186 5.6,8.53333 C5.6,9.04807 6.01813,9.46667 6.53333,9.46667 L7.46667,9.46667 C9.5284,9.46667 11.2,7.79507 11.2,5.73333 C11.2,3.6716 9.5284,2 7.46667,2 L3.73333,2 C1.6716,2 0,3.6716 0,5.73333 C0,7.124 0.762067,8.33453 1.88953,8.97713 C1.87553,8.83107 1.86667,8.6836 1.86667,8.53333 C1.86667,7.92013 1.98753,7.33447 2.2036,6.7978 C1.99267,6.4954 1.86667,6.12953 1.86667,5.73333 C1.86667,4.70387 2.70387,3.86667 3.73333,3.86667 Z M12.1095,5.28907 C12.1231,5.4356 12.1333,5.58307 12.1333,5.73333 C12.1333,6.34607 12.0101,6.9294 11.7931,7.46513 C12.0059,7.768 12.1333,8.13573 12.1333,8.53333 C12.1333,9.5628 11.2961,10.4 10.2667,10.4 L6.53333,10.4 C5.50387,10.4 4.66667,9.5628 4.66667,8.53333 C4.66667,7.50387 5.50387,6.66667 6.53333,6.66667 L7.46667,6.66667 C7.98187,6.66667 8.4,6.24807 8.4,5.73333 C8.4,5.2186 7.98187,4.8 7.46667,4.8 L6.53333,4.8 C4.4716,4.8 2.8,6.4716 2.8,8.53333 C2.8,10.59507 4.4716,12.2667 6.53333,12.2667 L10.2667,12.2667 C12.3284,12.2667 14,10.59507 14,8.53333 C14,7.14267 13.2375,5.93167 12.1095,5.28907 Z"></path></svg></span>진행시 참고링크</th><td></td></tr><tr class="property-row property-row-multi_select"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesMultipleSelect"><path d="M4,3 C4,2.447715 4.447715,2 5,2 L12,2 C12.5523,2 13,2.447716 13,3 C13,3.55228 12.5523,4 12,4 L5,4 C4.447715,4 4,3.55228 4,3 Z M4,7 C4,6.447715 4.447715,6 5,6 L12,6 C12.5523,6 13,6.447716 13,7 C13,7.55228 12.5523,8 12,8 L5,8 C4.447715,8 4,7.55228 4,7 Z M4,11 C4,10.447715 4.447715,10 5,10 L12,10 C12.5523,10 13,10.447716 13,11 C13,11.55228 12.5523,12 12,12 L5,12 C4.447715,12 4,11.55228 4,11 Z M2,4 C1.44771525,4 1,3.55228475 1,3 C1,2.44771525 1.44771525,2 2,2 C2.55228475,2 3,2.44771525 3,3 C3,3.55228475 2.55228475,4 2,4 Z M2,8 C1.44771525,8 1,7.55228475 1,7 C1,6.44771525 1.44771525,6 2,6 C2.55228475,6 3,6.44771525 3,7 C3,7.55228475 2.55228475,8 2,8 Z M2,12 C1.44771525,12 1,11.5522847 1,11 C1,10.4477153 1.44771525,10 2,10 C2.55228475,10 3,10.4477153 3,11 C3,11.5522847 2.55228475,12 2,12 Z"></path></svg></span>키워드</th><td></td></tr><tr class="property-row property-row-file"><th><span class="icon property-icon"><svg viewBox="0 0 14 14" style="width:14px;height:14px;display:block;fill:rgba(55, 53, 47, 0.4);flex-shrink:0;-webkit-backface-visibility:hidden" class="typesFile"><path d="M5.94578,14 C4.62416,14 3.38248,13.4963 2.44892,12.585 C1.514641,11.6736 1,10.4639 1,9.17405 C1.00086108,7.88562 1.514641,6.67434 2.44892,5.76378 L7.45612,0.985988 C8.80142,-0.327216 11.1777,-0.332396 12.5354,0.992848 C13.9369,2.36163 13.9369,4.58722 12.5354,5.95418 L8.03046,10.2414 C7.16278,11.0877 5.73682,11.0894 4.86024,10.2345 C3.98394,9.37789 3.98394,7.98769 4.86024,7.1327 L6.60422,5.4317 L7.87576,6.67196 L6.13177,8.37297 C6.01668,8.48539 6.00003,8.61545 6.00003,8.68335 C6.00003,8.75083 6.01668,8.88103 6.13177,8.99429 C6.36197,9.21689 6.53749,9.21689 6.76768,8.99429 L11.2707,4.70622 C11.9645,4.03016 11.9645,2.91757 11.2638,2.23311 C10.5843,1.57007 9.40045,1.57007 8.72077,2.23311 L3.71342,7.0109 C3.12602,7.58406 2.79837,8.35435 2.79837,9.17405 C2.79837,9.99459 3.12602,10.7654 3.72045,11.3446 C4.90947,12.5062 6.98195,12.5062 8.17096,11.3446 L10.41911,9.15165 L11.6906,10.3919 L9.4425,12.585 C8.50808,13.4963 7.2664,14 5.94578,14 Z"></path></svg></span>파일</th><td></td></tr></tbody></table></header><div class="page-body"><p id="a59f3cd1-864e-49d0-9986-a2fdfcd6ddca" class="">앞서, 토큰화에 대해 알아보았다. 이때, 우리는 다양한 단어들을 형태소로 분할하여 분석할 수 있음을 알게 됐다. 형태소마다 분할하여, 컴퓨터가 각 형태소의 차이를 인지할 수 있다. 이로써 우리는 텍스트를 분석할 수 있게 됐다!</p><p id="fc66eafa-cceb-4f78-a094-870c023e6cba" class="">그런데, 우리는 아직 단어 간의 연관성까지는 알아낼  수 없다. 가령, &#x27;terrible&#x27;와 &#x27;love&#x27;, &#x27;cute&#x27; 단어의 차이가 동일하다고 인식하는 것이다. 이러한 연관성 이슈는 <strong>벡터라이제이션을 통해 해결이 가능</strong>하다. 따라서 우리는, 토큰화를 한 이후 벡터라이제이션을 학습해야 한다.</p><h1 id="928013e8-9d10-4dec-a7fa-fb5abdb6da7d" class="">벡터화(Vectorization)</h1><h2 id="d2c162a3-0eb8-4122-a1b3-ccfcbc653e32" class="">벡터화 종류</h2><ul id="514a328b-5218-418e-bcee-38f6ff01704f" class="bulleted-list"><li>BoW</li></ul><ul id="06577e12-7d67-4cb2-9978-f0c197362a87" class="bulleted-list"><li>TF-IDF</li></ul><p id="2c7f1cf7-0bdc-4da7-adfe-5696c2036476" class="">벡터라이제이션 방법들 중, 이번 포스팅에서는 BoW &amp; TF-IDF에 대한 개념을 살펴볼 것이다.</p><p id="3764a0c3-4c07-4886-a0a6-d887f2ab58c1" class="">
</p><h3 id="0f3a8ac0-191d-4d38-b7d5-c0eb9f36a6be" class="">BoW</h3><p id="175d4805-605f-42a5-9450-569ecab7635f" class="">토큰의 빈도로 벡터화하는 방법이다. 문맥의 순서를 무시하고, 빈도값으로 피처를 추출하는 방법이다. 문장 1과 문장 2에 있는 중복을 제거한 뒤, 위치 인덱스 값을 부여한다. 이는 쉽고 빠른 구축이 가능하며, 여러 분야의 활용도가 높다. 하지만 <strong>문맥의미를 반영 이슈</strong>와 <strong>희소행렬 이슈</strong>가 있다.</p><h3 id="cb2c410b-36ec-4630-879f-a5cd64a34259" class="">TF - IDF</h3><p id="b536f3da-22ab-4d52-8c0a-4b0f31cc9cdc" class="">Term Frequency Inverse Document Frequency를 뜻한다. <strong>단어 간 연관성을 알고 싶을 때 사용</strong>하는 방법이다.</p><p id="675edc5f-2055-4b0f-b77b-a01e58842a95" class="">
</p><figure id="6a4fffb3-c2f9-427f-96c1-9db36eaf7df6" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn><mo>+</mo><mfrac><mrow><mn>0.5</mn><mo>×</mo><mi mathvariant="normal">f</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">{</mo><mi mathvariant="normal">f</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>:</mo><mi>w</mi><mo>∈</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">{\mathrm {tf}}(t,d)=0.5+{\frac {0.5\times {\mathrm {f}}(t,d)}{\max\{{\mathrm {f}}(w,d):w\in d\}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">t</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">max</span><span class="mopen">{</span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">d</span><span class="mclose">}</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></div></figure><p id="82ab0a52-6698-4e0a-8b41-98e967aaae29" class="">Term Frequency는 <strong>특정 문서 d에서 특정 단어 t가 쓰인 빈도</strong>를 뜻한다. 문서에서 여러 번 출현하면 연관성이 높을 것이라고 가정한다. 즉, 한 문장에서 여러 번 출현하면 그 단어가 중요한 단어라고 인식하는 것이다. 이를 혼자만 사용하는 것은 <strong>불용어를 중요하게 인식하게 될 수 있다는 문제</strong>를 야기할 수 있으므로, IDF와 함께 곱해 사용한다. </p><p id="f7e59dca-cd23-4c21-9f4b-a2c704267b63" class="">
</p><figure id="280d0ec1-80e9-4f86-ac10-ea7b2f79b991" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><mi>D</mi><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">{</mo><mi>d</mi><mo>∈</mo><mi>D</mi><mo>:</mo><mi>t</mi><mo>∈</mo><mi>d</mi><mo stretchy="false">}</mo><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">{\mathrm {idf}}(t,D)=\log {\frac {|D|}{|\{d\in D:t\in d\}|}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mopen">{</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">d</span><span class="mclose">}</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></div></figure><p id="01c7a160-8c7f-42e1-8ab0-20c1e0033240" class="">Inverse Document Frequency는 로그 안에서 출현한 <strong>총 단어의 갯수</strong>를 단어가 출현한 <strong>총 문장</strong>의 갯수로  나눠 주는 것이다. 이는 <strong>여러 문서에서 많이 등장하는 단어일 수록 중요도가 낮다고 판단</strong>한다.</p><p id="cda39a79-1116-4a3c-8e7b-de648d2cddb2" class="">이 TF값과 IDF값을 곱해주면, 문장의 연관성을 수치로 나타낼 수 있게 된다.</p><figure id="bdb2690b-7654-423a-9c5e-b6a80bfb8117" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">D</mi><mi mathvariant="normal">F</mi><mo>=</mo><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>×</mo><mrow><mi>t</mi><mi>f</mi></mrow></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\mathrm {TF\_IDF={idf}}(t,D) \times  {tf}}(t,D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">T</span><span class="mord mathrm">F</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathrm">I</span><span class="mord mathrm">D</span><span class="mord mathrm">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="b7a5d6c8-d70d-452c-8b47-f72ab6eb6b4e" class="">참고링크: <a href="https://www.youtube.com/watch?v=meEchvkdB1U">https://www.youtube.com/watch?v=meEchvkdB1U</a></p><p id="fa27a3ba-c85a-42a3-8c8a-16c40b60d320" class="">
</p><p id="17accfbc-0cce-49a5-abc3-b6c429961479" class="">
</p><p id="196cb6ad-f389-432b-b3bc-18b4642be228" class="">백문이 불여일견, 백견이 불여일행이다. 한번 예제를 살펴보자.</p><p id="5427d03e-a25b-4212-a5ed-387c54ead955" class="">
</p><p id="1afa82ef-0072-48f6-99a8-3bb6ee05243b" class="">
</p><p id="1e4f87fd-6174-4499-8a6a-680d16294425" class="">우리는 <strong>TEXT 문서의 토큰 별 벡터값을 산출</strong>할 것이다. 이때, 벡터라이제이션 방법론은 &#x27;TF-IDF&#x27; 방법이다.</p><p id="4151a49d-2aff-42ec-bc8b-5136bb77e324" class="">
</p><p id="6cf26d05-e46c-4f69-89ba-96507008f3c5" class=""><strong>1. Lib import</strong></p><p id="cc4dbdb7-15f5-4510-a629-0cc0e54bca1a" class="">필요한 라이브러리를 import 해준다.</p><pre id="2da05cba-4bb3-49f5-93c1-8106247d4197" class="code"><code>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import spacy
import pandas as pd</code></pre><p id="12792504-f97e-414d-8b9a-0a7db92c0cca" class="">
</p><p id="32d0ff63-b8e2-48cf-9442-6370de99df6e" class=""><strong>2. Test 문서 지정</strong></p><p id="b39a8f35-90e3-436e-9139-e7882be7f21c" class="">나는 우리집 둘째 강아지 엔느의 이야기를 index 0에 작성했고, 막내 강아지 향단이의 이야기를 index 1에 작성했다. 테스트 문서기 때문에 그냥 요러쿵 저러쿵 써봤다 ㅋㅋ.</p><pre id="49098e1d-9a2c-43a3-aee7-5bfbc0e2d66e" class="code"><code># 테스트 문서 (D)
text =[&quot;Hello, this is Sohee. Let me introduce my puppy. My puppy&#x27;s name is Enne. It is 14 years old. Yes, it&#x27;s too old. But it is very healthy. By the way it has one problem. However, he suffers from constipation. (So it eats a lot of bananas to get rid of constipation.) I think It&#x27;s constipated because it doesn&#x27;t exercise often. So I try to walk with it often to clear up his constipation. Walking is the best exercise for my lovely puppy!&quot;,
       &quot;The second puppy&#x27;s name is Hyangdan. Hyangdan is a really bad puppy. Because it often hits other puppies. For example, when I think my family doesn&#x27;t seem to be interested in it, it gets very upset. And hit the other puppies. That&#x27;s why I think it&#x27;s naughty. However, it has a cute appearance, so it is so much loved by Dad. I really want my dad to scold it often.&quot;]</code></pre><p id="0c4eae24-26bd-4c3e-8f96-d73de4255081" class="">
</p><p id="190c7fd2-1206-4f81-94fd-07ba64ae7d58" class=""><strong>3-1. TF-IDF 적용</strong></p><p id="505d9657-035d-40b7-bbda-19e7eab42aad" class="">우선, 아무 parameter 조정 없이 TF-IDF를 적용해보자.</p><p id="5f287207-d438-4012-94ea-33637b428a0f" class="">이때, 나는 max_features는 15로 지정했다. 이는 최대 feature수를 뜻한다. feature 수는 token 수 만큼 할당되는데, 만약 특정 문서 d만 feature가 1000(외 나머지 문서 feature는 평균 10)이면, 모든 feature수가 1000으로 정해지고, 이는 <strong>모델 성능 저하</strong>를 야기할 수 있다. 따라서 <strong>max_features를 정의해주는 것이 좋다. 이는 빈도 순서대로 top n 토큰만 사용된다는 장점</strong>이 있다. 따라서 <strong>사전 EDA 통해 데이터 length를 살펴본 뒤, max_feature를 적절히 설정</strong>해주는 것이 중요하다.</p><pre id="4b18dec8-6837-4631-94b8-31d874e5adb9" class="code"><code># TF-IDF 적용

# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한
tfidf = TfidfVectorizer(stop_words=&#x27;english&#x27;, max_features=15)

# Fit 후 dtm 생성(문서, 단어마다 tf-idf 값을 계산합니다)
dtm = tfidf.fit_transform(text)

dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())
dtm</code></pre><p id="459dfc0a-1fee-48b9-b754-cb57802fcaa8" class="">
</p><p id="11188e10-f500-4c09-b985-f58b27de9c00" class="">결과 값은 아래와 같다. 엔느의 이야기였던 인덱스 0에는 변비(ㅋㅋㅋ)가 가장 중요한 값으로 지정됐다. 내가 &#x27;엔느는 변비때문에 바나나 먹고, 산책해야 한다&#x27;고 강조했기 때문인걸까. 압도적으로 높은 vector 값이 나왔다.</p><p id="0b45df14-5898-420a-bff1-1327a8c9b308" class="">인덱스 1에는 향단이의 이야기였는데, 대명사 필터링이 되지 않아서 그런지 본인의 이름(향단)이 참 높은 vector값이 나왔다. 그리고, dad의 vactor 값이 높게 나왔다. 비록 &#x27;dad&#x27;는 한 번 밖에 언급하지 않았지만, 두 문장 중 유일하게 언급됐기 때문에 높은 vector값이 산출된 것을 알 수 있다.</p><p id="46069dc8-ca85-42f3-be89-15448926f2b6" class="">
</p><figure id="9579c4b0-2218-417e-9996-7c9d6c06c6dd" class="image"><a href="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled.png"><img style="width:1770px" src="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled.png"/></a></figure><p id="88f05d21-d41c-4d28-b595-e8040b57e56b" class="">
</p><p id="85b74987-e0f3-4698-bfc5-9226bdb0b6c0" class="">
</p><p id="78666e33-98d4-4add-8d7e-5004e8ffe4d2" class="">위와 같이, 대명사가 필터링되지 않았고 &amp; 의미가 같지만 어미가 다른 단어(예: puppy와 puppies)가 다른 토큰으로 인식되는 등의 이슈가 발생한 것을 알 수 있다. 따라서 이를 <strong>파라미터 튜닝</strong>을 통해, 해결해볼 것이다.</p><p id="bbbb107e-b2b7-45e1-a967-59d4cb7df8e8" class="">
</p><p id="0e74055c-041e-4f28-8d83-dc30f84e7949" class=""><strong>3-2. 파라미터 튜닝을 위한 nlp, stop_words 지정</strong></p><p id="483815bb-0b3b-4180-b947-fc9fc7f7aba8" class=""><mark class="highlight-gray">(※ 이 부분에서, STOP_WORDS(추가 불용어 사전)이 제대로 적용되지 않아 모델링에서는 배제했다. 해결해야 한다, 문제 해결 시 수정 예정!)</mark></p><pre id="7dbcf058-c1f1-4a05-9657-ee2053499129" class="code"><code># 파라미터 튜닝 이후 TF-IDF 진행

nlp = spacy.load(&quot;en_core_web_sm&quot;)
# 불용어 추가 이슈 해결 X : STOP_WORDS = [&quot;&#x27;&quot;, &quot; &quot;, &quot;sohee&quot;, &quot;yes&quot;, &quot;other&quot;]</code></pre><pre id="379e134e-4b0e-41dc-92ea-6a6b0c2d86c8" class="code"><code># spacy tokenizer 함수 사용
def tokenize(document):    
    doc = nlp(document)
    # punctuations: !&quot;#$%&amp;&#x27;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~
    # pos : 대명사인지의 여부

    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.pos_ != &#x27;PROPN&#x27;)]</code></pre><pre id="192ba03f-bdd2-44a6-984d-a4e3208dc1da" class="code"><code>tfidf = TfidfVectorizer(stop_words=&#x27;english&#x27;
                        ,tokenizer=tokenize
                        ,max_features=15)

dtm = tfidf.fit_transform(text)

dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())
dtm</code></pre><figure id="04b2a0c1-c86e-49c8-aedb-a880aeb3e253" class="image"><a href="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled%201.png"><img style="width:1436px" src="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled%201.png"/></a></figure><p id="342c7a49-d7a0-4888-af6b-de48c031efef" class="">
</p><p id="d39ff75d-40b4-4499-8887-5c1846a19277" class="">&#x27;doesn&#x27; 등은 불용어로 인식하여 사라진 것을 알 수 있다. 그리고 &#x27;puppy&#x27;와 &#x27;puppies&#x27;는 같은 단어로 인식하고 있음을 확인할 수 있다. </p><p id="deb43cf3-ea58-4c78-879f-a1ada30b96f4" class="">또한 대명사를 삭제하도록 parameter 튜닝을 했기 때문에, &#x27;hyandan&#x27;은 없어졌다. ( &#x27;sohee&#x27;는 한글 명이라서, 대명사로 인식을 제대로 못 한 것 같다. 이런 것들의 경우, 불용어 추가(STOP_WORDS) 기능을 활용해 없애 주어야 한다.)</p><p id="4c02062d-f7ff-4d61-9e8e-b96e84971d0a" class="">
</p><p id="86b91d6b-2ffc-47bb-baf4-a03db5a463c0" class=""><mark class="highlight-gray"><em>이로써 TF-IDF 예제에 대해 살펴봤습니다. 앞으로는 단어의 개수 기반이 아닌,  </em></mark><mark class="highlight-gray"><em><strong>단어 간의 유사도를 반영</strong></em></mark><mark class="highlight-gray"><em>하는 방법론에 대해 포스팅해보겠습니다, 긴 글 읽어주셔서 감사합니다!</em></mark></p><p id="b9a79df1-1bc6-404e-bfda-5acd3b16a669" class=""><mark class="highlight-gray"><em>e-mail : eng.sohee@gmail.com</em></mark></p><p id="14968f3d-b9f1-4dfa-8d3a-23024204ccf7" class=""> </p></div></article></body>
