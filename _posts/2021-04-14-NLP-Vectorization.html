---
layout: post
title: "[DL] 04. NLP - 벡터화(Vectorization)"
subtitle: "BoW와 TF-IDF, spacy, sklearn 활용한 TF-IDF 실습"
date: 2021-04-14 10:45:13
background: '/img/posts/DL_01_텍스트마이닝사진.png'
categories: ['Deep Learning']
---


<p id="a59f3cd1-864e-49d0-9986-a2fdfcd6ddca" class="">앞서, 토큰화에 대해 알아보았다. 이때, 우리는 다양한 단어들을 형태소로 분할하여 분석할 수 있음을 알게 됐다. 형태소마다 분할하여, 컴퓨터가 각 형태소의 차이를 인지할 수 있다. 이로써 우리는 텍스트를 분석할 수 있게 됐다!</p><p id="fc66eafa-cceb-4f78-a094-870c023e6cba" class="">그런데, 우리는 아직 단어 간의 연관성까지는 알아낼  수 없다. 가령, &#x27;terrible&#x27;와 &#x27;love&#x27;, &#x27;cute&#x27; 단어의 차이가 동일하다고 인식하는 것이다. 이러한 연관성 이슈는 <strong>벡터라이제이션을 통해 해결이 가능</strong>하다. 따라서 우리는, 토큰화를 한 이후 벡터라이제이션을 학습해야 한다.</p><h1 id="928013e8-9d10-4dec-a7fa-fb5abdb6da7d" class="">벡터화(Vectorization)</h1><h2 id="d2c162a3-0eb8-4122-a1b3-ccfcbc653e32" class="">벡터화 종류</h2><ul id="514a328b-5218-418e-bcee-38f6ff01704f" class="bulleted-list"><li>BoW</li></ul><ul id="06577e12-7d67-4cb2-9978-f0c197362a87" class="bulleted-list"><li>TF-IDF</li></ul><p id="2c7f1cf7-0bdc-4da7-adfe-5696c2036476" class="">벡터라이제이션 방법들 중, 이번 포스팅에서는 BoW &amp; TF-IDF에 대한 개념을 살펴볼 것이다.</p><p id="3764a0c3-4c07-4886-a0a6-d887f2ab58c1" class="">
</p><h3 id="0f3a8ac0-191d-4d38-b7d5-c0eb9f36a6be" class="">BoW</h3><p id="175d4805-605f-42a5-9450-569ecab7635f" class="">토큰의 빈도로 벡터화하는 방법이다. 문맥의 순서를 무시하고, 빈도값으로 피처를 추출하는 방법이다. 문장 1과 문장 2에 있는 중복을 제거한 뒤, 위치 인덱스 값을 부여한다. 이는 쉽고 빠른 구축이 가능하며, 여러 분야의 활용도가 높다. 하지만 <strong>문맥의미를 반영 이슈</strong>와 <strong>희소행렬 이슈</strong>가 있다.</p><h3 id="cb2c410b-36ec-4630-879f-a5cd64a34259" class="">TF - IDF</h3><p id="b536f3da-22ab-4d52-8c0a-4b0f31cc9cdc" class="">Term Frequency Inverse Document Frequency를 뜻한다. <strong>단어 간 연관성을 알고 싶을 때 사용</strong>하는 방법이다.</p><p id="675edc5f-2055-4b0f-b77b-a01e58842a95" class="">
</p><figure id="6a4fffb3-c2f9-427f-96c1-9db36eaf7df6" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">t</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0.5</mn><mo>+</mo><mfrac><mrow><mn>0.5</mn><mo>×</mo><mi mathvariant="normal">f</mi><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">{</mo><mi mathvariant="normal">f</mi><mo stretchy="false">(</mo><mi>w</mi><mo separator="true">,</mo><mi>d</mi><mo stretchy="false">)</mo><mo>:</mo><mi>w</mi><mo>∈</mo><mi>d</mi><mo stretchy="false">}</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">{\mathrm {tf}}(t,d)=0.5+{\frac {0.5\times {\mathrm {f}}(t,d)}{\max\{{\mathrm {f}}(w,d):w\in d\}}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">t</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.72777em;vertical-align:-0.08333em;"></span><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop">max</span><span class="mopen">{</span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">d</span><span class="mclose">}</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">0</span><span class="mord">.</span><span class="mord">5</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord"><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault">d</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></div></figure><p id="82ab0a52-6698-4e0a-8b41-98e967aaae29" class="">Term Frequency는 <strong>특정 문서 d에서 특정 단어 t가 쓰인 빈도</strong>를 뜻한다. 문서에서 여러 번 출현하면 연관성이 높을 것이라고 가정한다. 즉, 한 문장에서 여러 번 출현하면 그 단어가 중요한 단어라고 인식하는 것이다. 이를 혼자만 사용하는 것은 <strong>불용어를 중요하게 인식하게 될 수 있다는 문제</strong>를 야기할 수 있으므로, IDF와 함께 곱해 사용한다. </p><p id="f7e59dca-cd23-4c21-9f4b-a2c704267b63" class="">
</p><figure id="280d0ec1-80e9-4f86-ac10-ea7b2f79b991" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><mi>D</mi><mi mathvariant="normal">∣</mi></mrow><mrow><mi mathvariant="normal">∣</mi><mo stretchy="false">{</mo><mi>d</mi><mo>∈</mo><mi>D</mi><mo>:</mo><mi>t</mi><mo>∈</mo><mi>d</mi><mo stretchy="false">}</mo><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">{\mathrm {idf}}(t,D)=\log {\frac {|D|}{|\{d\in D:t\in d\}|}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mopen">{</span><span class="mord mathdefault">d</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">t</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathdefault">d</span><span class="mclose">}</span><span class="mord">∣</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mord">∣</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></span></div></figure><p id="01c7a160-8c7f-42e1-8ab0-20c1e0033240" class="">Inverse Document Frequency는 로그 안에서 출현한 <strong>총 단어의 갯수</strong>를 단어가 출현한 <strong>총 문장</strong>의 갯수로  나눠 주는 것이다. 이는 <strong>여러 문서에서 많이 등장하는 단어일 수록 중요도가 낮다고 판단</strong>한다.</p><p id="cda39a79-1116-4a3c-8e7b-de648d2cddb2" class="">이 TF값과 IDF값을 곱해주면, 문장의 연관성을 수치로 나타낼 수 있게 된다.</p><figure id="bdb2690b-7654-423a-9c5e-b6a80bfb8117" class="equation"><style>@import url('https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css')</style><div class="equation-container"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mrow><mi mathvariant="normal">T</mi><mi mathvariant="normal">F</mi><mi mathvariant="normal">_</mi><mi mathvariant="normal">I</mi><mi mathvariant="normal">D</mi><mi mathvariant="normal">F</mi><mo>=</mo><mrow><mi mathvariant="normal">i</mi><mi mathvariant="normal">d</mi><mi mathvariant="normal">f</mi></mrow></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo><mo>×</mo><mrow><mi>t</mi><mi>f</mi></mrow></mrow><mo stretchy="false">(</mo><mi>t</mi><mo separator="true">,</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">{\mathrm {TF\_IDF={idf}}(t,D) \times  {tf}}(t,D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.06em;vertical-align:-0.31em;"></span><span class="mord"><span class="mord"><span class="mord mathrm">T</span><span class="mord mathrm">F</span><span class="mord mathrm" style="margin-right:0.02778em;">_</span><span class="mord mathrm">I</span><span class="mord mathrm">D</span><span class="mord mathrm">F</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mord mathrm">i</span><span class="mord mathrm">d</span><span class="mord mathrm" style="margin-right:0.07778em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathdefault">t</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span></span></span><span class="mopen">(</span><span class="mord mathdefault">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span><span class="mclose">)</span></span></span></span></span></div></figure><p id="b7a5d6c8-d70d-452c-8b47-f72ab6eb6b4e" class="">참고링크: <a href="https://www.youtube.com/watch?v=meEchvkdB1U">https://www.youtube.com/watch?v=meEchvkdB1U</a></p><p id="fa27a3ba-c85a-42a3-8c8a-16c40b60d320" class="">
</p><p id="17accfbc-0cce-49a5-abc3-b6c429961479" class="">
</p><p id="196cb6ad-f389-432b-b3bc-18b4642be228" class="">백문이 불여일견, 백견이 불여일행이다. 한번 예제를 살펴보자.</p><p id="5427d03e-a25b-4212-a5ed-387c54ead955" class="">
</p><p id="1afa82ef-0072-48f6-99a8-3bb6ee05243b" class="">
</p><p id="1e4f87fd-6174-4499-8a6a-680d16294425" class="">우리는 <strong>TEXT 문서의 토큰 별 벡터값을 산출</strong>할 것이다. 이때, 벡터라이제이션 방법론은 &#x27;TF-IDF&#x27; 방법이다.</p><p id="4151a49d-2aff-42ec-bc8b-5136bb77e324" class="">
</p><p id="6cf26d05-e46c-4f69-89ba-96507008f3c5" class=""><strong>1. Lib import</strong></p><p id="cc4dbdb7-15f5-4510-a629-0cc0e54bca1a" class="">필요한 라이브러리를 import 해준다.</p><pre id="2da05cba-4bb3-49f5-93c1-8106247d4197" class="code"><pre><code>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import spacy
import pandas as pd</code></pre></pre><p id="12792504-f97e-414d-8b9a-0a7db92c0cca" class="">
</p><p id="32d0ff63-b8e2-48cf-9442-6370de99df6e" class=""><strong>2. Test 문서 지정</strong></p><p id="b39a8f35-90e3-436e-9139-e7882be7f21c" class="">나는 우리집 둘째 강아지 엔느의 이야기를 index 0에 작성했고, 막내 강아지 향단이의 이야기를 index 1에 작성했다. 테스트 문서기 때문에 그냥 요러쿵 저러쿵 써봤다 ㅋㅋ.</p><pre id="49098e1d-9a2c-43a3-aee7-5bfbc0e2d66e" class="code"><pre><code># 테스트 문서 (D)
text =[&quot;Hello, this is Sohee. Let me introduce my puppy. My puppy&#x27;s name is Enne. It is 14 years old. Yes, it&#x27;s too old. But it is very healthy. By the way it has one problem. However, he suffers from constipation. (So it eats a lot of bananas to get rid of constipation.) I think It&#x27;s constipated because it doesn&#x27;t exercise often. So I try to walk with it often to clear up his constipation. Walking is the best exercise for my lovely puppy!&quot;,
       &quot;The second puppy&#x27;s name is Hyangdan. Hyangdan is a really bad puppy. Because it often hits other puppies. For example, when I think my family doesn&#x27;t seem to be interested in it, it gets very upset. And hit the other puppies. That&#x27;s why I think it&#x27;s naughty. However, it has a cute appearance, so it is so much loved by Dad. I really want my dad to scold it often.&quot;]</code></pre></pre><p id="0c4eae24-26bd-4c3e-8f96-d73de4255081" class="">
</p><p id="190c7fd2-1206-4f81-94fd-07ba64ae7d58" class=""><strong>3-1. TF-IDF 적용</strong></p><p id="505d9657-035d-40b7-bbda-19e7eab42aad" class="">우선, 아무 parameter 조정 없이 TF-IDF를 적용해보자.</p><p id="5f287207-d438-4012-94ea-33637b428a0f" class="">이때, 나는 max_features는 15로 지정했다. 이는 최대 feature수를 뜻한다. feature 수는 token 수 만큼 할당되는데, 만약 특정 문서 d만 feature가 1000(외 나머지 문서 feature는 평균 10)이면, 모든 feature수가 1000으로 정해지고, 이는 <strong>모델 성능 저하</strong>를 야기할 수 있다. 따라서 <strong>max_features를 정의해주는 것이 좋다. 이는 빈도 순서대로 top n 토큰만 사용된다는 장점</strong>이 있다. 따라서 <strong>사전 EDA 통해 데이터 length를 살펴본 뒤, max_feature를 적절히 설정</strong>해주는 것이 중요하다.</p><pre id="4b18dec8-6837-4631-94b8-31d874e5adb9" class="code"><pre><code># TF-IDF 적용

# TF-IDF vectorizer. 테이블을 작게 만들기 위해 max_features=15로 제한
tfidf = TfidfVectorizer(stop_words=&#x27;english&#x27;, max_features=15)

# Fit 후 dtm 생성(문서, 단어마다 tf-idf 값을 계산합니다)
dtm = tfidf.fit_transform(text)

dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())
dtm</code></pre><p id="459dfc0a-1fee-48b9-b754-cb57802fcaa8" class="">
</p><p id="11188e10-f500-4c09-b985-f58b27de9c00" class="">결과 값은 아래와 같다. 엔느의 이야기였던 인덱스 0에는 변비(ㅋㅋㅋ)가 가장 중요한 값으로 지정됐다. 내가 &#x27;엔느는 변비때문에 바나나 먹고, 산책해야 한다&#x27;고 강조했기 때문인걸까. 압도적으로 높은 vector 값이 나왔다.</p><p id="0b45df14-5898-420a-bff1-1327a8c9b308" class="">인덱스 1에는 향단이의 이야기였는데, 대명사 필터링이 되지 않아서 그런지 본인의 이름(향단)이 참 높은 vector값이 나왔다. 그리고, dad의 vactor 값이 높게 나왔다. 비록 &#x27;dad&#x27;는 한 번 밖에 언급하지 않았지만, 두 문장 중 유일하게 언급됐기 때문에 높은 vector값이 산출된 것을 알 수 있다.</p><p id="46069dc8-ca85-42f3-be89-15448926f2b6" class="">
</p><figure id="9579c4b0-2218-417e-9996-7c9d6c06c6dd" class="image"><a href="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled.png"><img style="width:1770px" src="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled.png"/></a></figure><p id="88f05d21-d41c-4d28-b595-e8040b57e56b" class="">
</p><p id="85b74987-e0f3-4698-bfc5-9226bdb0b6c0" class="">
</p><p id="78666e33-98d4-4add-8d7e-5004e8ffe4d2" class="">위와 같이, 대명사가 필터링되지 않았고 &amp; 의미가 같지만 어미가 다른 단어(예: puppy와 puppies)가 다른 토큰으로 인식되는 등의 이슈가 발생한 것을 알 수 있다. 따라서 이를 <strong>파라미터 튜닝</strong>을 통해, 해결해볼 것이다.</p><p id="bbbb107e-b2b7-45e1-a967-59d4cb7df8e8" class="">
</p><p id="0e74055c-041e-4f28-8d83-dc30f84e7949" class=""><strong>3-2. 파라미터 튜닝을 위한 nlp, stop_words 지정</strong></p><p id="483815bb-0b3b-4180-b947-fc9fc7f7aba8" class=""><mark class="highlight-gray">(※ 이 부분에서, STOP_WORDS(추가 불용어 사전)이 제대로 적용되지 않아 모델링에서는 배제했다. 해결해야 한다, 문제 해결 시 수정 예정!)</mark></p><pre id="7dbcf058-c1f1-4a05-9657-ee2053499129" class="code"><code># 파라미터 튜닝 이후 TF-IDF 진행

nlp = spacy.load(&quot;en_core_web_sm&quot;)
# 불용어 추가 이슈 해결 X : STOP_WORDS = [&quot;&#x27;&quot;, &quot; &quot;, &quot;sohee&quot;, &quot;yes&quot;, &quot;other&quot;]</code></pre><pre id="379e134e-4b0e-41dc-92ea-6a6b0c2d86c8" class="code"><code># spacy tokenizer 함수 사용
def tokenize(document):    
    doc = nlp(document)
    # punctuations: !&quot;#$%&amp;&#x27;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~
    # pos : 대명사인지의 여부

    return [token.lemma_.strip() for token in doc if (token.is_stop != True) and (token.is_punct != True) and (token.pos_ != &#x27;PROPN&#x27;)]</code></pre><pre id="192ba03f-bdd2-44a6-984d-a4e3208dc1da" class="code"><code>tfidf = TfidfVectorizer(stop_words=&#x27;english&#x27;
                        ,tokenizer=tokenize
                        ,max_features=15)

dtm = tfidf.fit_transform(text)

dtm = pd.DataFrame(dtm.todense(), columns=tfidf.get_feature_names())
dtm</code></pre><figure id="04b2a0c1-c86e-49c8-aedb-a880aeb3e253" class="image"><a href="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled%201.png"><img style="width:1436px" src="%E1%84%87%E1%85%A6%E1%86%A8%E1%84%90%E1%85%A5%E1%84%85%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%89%E1%85%A7%E1%86%AB%209579c4b02218417e99967c9d6c06c6dd/Untitled%201.png"/></a></figure><p id="342c7a49-d7a0-4888-af6b-de48c031efef" class="">
</p><p id="d39ff75d-40b4-4499-8887-5c1846a19277" class="">&#x27;doesn&#x27; 등은 불용어로 인식하여 사라진 것을 알 수 있다. 그리고 &#x27;puppy&#x27;와 &#x27;puppies&#x27;는 같은 단어로 인식하고 있음을 확인할 수 있다. </p><p id="deb43cf3-ea58-4c78-879f-a1ada30b96f4" class="">또한 대명사를 삭제하도록 parameter 튜닝을 했기 때문에, &#x27;hyandan&#x27;은 없어졌다. ( &#x27;sohee&#x27;는 한글 명이라서, 대명사로 인식을 제대로 못 한 것 같다. 이런 것들의 경우, 불용어 추가(STOP_WORDS) 기능을 활용해 없애 주어야 한다.)</p><p id="4c02062d-f7ff-4d61-9e8e-b96e84971d0a" class="">
</p><p id="86b91d6b-2ffc-47bb-baf4-a03db5a463c0" class=""><mark class="highlight-gray"><em>이로써 TF-IDF 예제에 대해 살펴봤습니다. 앞으로는 단어의 개수 기반이 아닌,  </em></mark><mark class="highlight-gray"><em><strong>단어 간의 유사도를 반영</strong></em></mark><mark class="highlight-gray"><em>하는 방법론에 대해 포스팅해보겠습니다, 긴 글 읽어주셔서 감사합니다!</em></mark></p><p id="b9a79df1-1bc6-404e-bfda-5acd3b16a669" class=""><mark class="highlight-gray"><em>e-mail : eng.sohee@gmail.com</em></mark></p><p id="14968f3d-b9f1-4dfa-8d3a-23024204ccf7" class=""> </p></div></article></body>
